<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>src.model.transformer package &mdash; Text2SQL 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Text2SQL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">src.model.transformer package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-src.model.transformer.activation">src.model.transformer.activation module</a><ul>
<li><a class="reference internal" href="#src.model.transformer.activation.RelationalMultiheadAttention"><code class="docutils literal notranslate"><span class="pre">RelationalMultiheadAttention</span></code></a><ul>
<li><a class="reference internal" href="#src.model.transformer.activation.RelationalMultiheadAttention.bias_k"><code class="docutils literal notranslate"><span class="pre">RelationalMultiheadAttention.bias_k</span></code></a></li>
<li><a class="reference internal" href="#src.model.transformer.activation.RelationalMultiheadAttention.bias_v"><code class="docutils literal notranslate"><span class="pre">RelationalMultiheadAttention.bias_v</span></code></a></li>
<li><a class="reference internal" href="#src.model.transformer.activation.RelationalMultiheadAttention.forward"><code class="docutils literal notranslate"><span class="pre">RelationalMultiheadAttention.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.model.transformer.encoder">src.model.transformer.encoder module</a><ul>
<li><a class="reference internal" href="#src.model.transformer.encoder.RelationalTransformerEncoder"><code class="docutils literal notranslate"><span class="pre">RelationalTransformerEncoder</span></code></a><ul>
<li><a class="reference internal" href="#src.model.transformer.encoder.RelationalTransformerEncoder.forward"><code class="docutils literal notranslate"><span class="pre">RelationalTransformerEncoder.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.model.transformer.encoder.RelationalTransformerEncoderLayer"><code class="docutils literal notranslate"><span class="pre">RelationalTransformerEncoderLayer</span></code></a><ul>
<li><a class="reference internal" href="#src.model.transformer.encoder.RelationalTransformerEncoderLayer.forward"><code class="docutils literal notranslate"><span class="pre">RelationalTransformerEncoderLayer.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.model.transformer.functional">src.model.transformer.functional module</a><ul>
<li><a class="reference internal" href="#src.model.transformer.functional.relational_multi_head_attention_forward"><code class="docutils literal notranslate"><span class="pre">relational_multi_head_attention_forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.model.transformer">Module contents</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Text2SQL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>src.model.transformer package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/src.model.transformer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="src-model-transformer-package">
<h1>src.model.transformer package<a class="headerlink" href="#src-model-transformer-package" title="Permalink to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-src.model.transformer.activation">
<span id="src-model-transformer-activation-module"></span><h2>src.model.transformer.activation module<a class="headerlink" href="#module-src.model.transformer.activation" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.model.transformer.activation.RelationalMultiheadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.model.transformer.activation.</span></span><span class="sig-name descname"><span class="pre">RelationalMultiheadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_relations</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_bias_kv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_zero_attn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.model.transformer.activation.RelationalMultiheadAttention" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Allows the model to jointly attend to information
from different representation subspaces as described in the paper:
<a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>.</p>
<p>Multi-Head Attention is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O\]</div>
<p>where <span class="math notranslate nohighlight">\(head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span>.</p>
<p><code class="docutils literal notranslate"><span class="pre">forward()</span></code> will use a special optimized implementation if all of the following
conditions are met:</p>
<ul class="simple">
<li><p>self attention is being computed (i.e., <code class="docutils literal notranslate"><span class="pre">query</span></code>, <code class="docutils literal notranslate"><span class="pre">key</span></code>, and <code class="docutils literal notranslate"><span class="pre">value</span></code> are the same tensor. This
restriction will be loosened in the future.)</p></li>
<li><p>Either autograd is disabled (using <code class="docutils literal notranslate"><span class="pre">torch.inference_mode</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.no_grad</span></code>) or no tensor argument <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code></p></li>
<li><p>training is disabled (using <code class="docutils literal notranslate"><span class="pre">.eval()</span></code>)</p></li>
<li><p>dropout is 0</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add_bias_kv</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add_zero_attn</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_first</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> and the input is batched</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kdim</span></code> and <code class="docutils literal notranslate"><span class="pre">vdim</span></code> are equal to <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code></p></li>
<li><p>at most one of <code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code> or <code class="docutils literal notranslate"><span class="pre">attn_mask</span></code> is passed</p></li>
<li><p>if a <a class="reference external" href="https://pytorch.org/docs/stable/nested.html">NestedTensor</a> is passed, neither <code class="docutils literal notranslate"><span class="pre">key_padding_mask</span></code>
nor <code class="docutils literal notranslate"><span class="pre">attn_mask</span></code> is passed</p></li>
</ul>
<p>If the optimized implementation is in use, a
<a class="reference external" href="https://pytorch.org/docs/stable/nested.html">NestedTensor</a> can be passed for
<code class="docutils literal notranslate"><span class="pre">query</span></code>/<code class="docutils literal notranslate"><span class="pre">key</span></code>/<code class="docutils literal notranslate"><span class="pre">value</span></code> to represent padding more efficiently than using a
padding mask. In this case, a <a class="reference external" href="https://pytorch.org/docs/stable/nested.html">NestedTensor</a>
will be returned, and an additional speedup proportional to the fraction of the input
that is padding can be expected.</p>
<dl>
<dt>Args:</dt><dd><p>embed_dim: Total dimension of the model.
num_heads: Number of parallel attention heads. Note that <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code> will be split</p>
<blockquote>
<div><p>across <code class="docutils literal notranslate"><span class="pre">num_heads</span></code> (i.e. each head will have dimension <code class="docutils literal notranslate"><span class="pre">embed_dim</span> <span class="pre">//</span> <span class="pre">num_heads</span></code>).</p>
</div></blockquote>
<p>dropout: Dropout probability on <code class="docutils literal notranslate"><span class="pre">attn_output_weights</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">0.0</span></code> (no dropout).
bias: If specified, adds bias to input / output projection layers. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.
add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.
add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.</p>
<blockquote>
<div><p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</div></blockquote>
<p>kdim: Total number of features for keys. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> (uses <code class="docutils literal notranslate"><span class="pre">kdim=embed_dim</span></code>).
vdim: Total number of features for values. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> (uses <code class="docutils literal notranslate"><span class="pre">vdim=embed_dim</span></code>).
batch_first: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the input and output tensors are provided</p>
<blockquote>
<div><p>as (batch, seq, feature). Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> (seq, batch, feature).</p>
</div></blockquote>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">multihead_attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RelationalMultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">multihead_attn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="src.model.transformer.activation.RelationalMultiheadAttention.bias_k">
<span class="sig-name descname"><span class="pre">bias_k</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#src.model.transformer.activation.RelationalMultiheadAttention.bias_k" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="src.model.transformer.activation.RelationalMultiheadAttention.bias_v">
<span class="sig-name descname"><span class="pre">bias_v</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#src.model.transformer.activation.RelationalMultiheadAttention.bias_v" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.model.transformer.activation.RelationalMultiheadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_padding_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_attn_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.model.transformer.activation.RelationalMultiheadAttention.forward" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>query: Query embeddings of shape <span class="math notranslate nohighlight">\((L, E_q)\)</span> for unbatched input, <span class="math notranslate nohighlight">\((L, N, E_q)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code></dt><dd><p>or <span class="math notranslate nohighlight">\((N, L, E_q)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>, where <span class="math notranslate nohighlight">\(L\)</span> is the target sequence length,
<span class="math notranslate nohighlight">\(N\)</span> is the batch size, and <span class="math notranslate nohighlight">\(E_q\)</span> is the query embedding dimension <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>.
Queries are compared against key-value pairs to produce the output.
See “Attention Is All You Need” for more details.</p>
</dd>
<dt>key: Key embeddings of shape <span class="math notranslate nohighlight">\((S, E_k)\)</span> for unbatched input, <span class="math notranslate nohighlight">\((S, N, E_k)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code></dt><dd><p>or <span class="math notranslate nohighlight">\((N, S, E_k)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>, where <span class="math notranslate nohighlight">\(S\)</span> is the source sequence length,
<span class="math notranslate nohighlight">\(N\)</span> is the batch size, and <span class="math notranslate nohighlight">\(E_k\)</span> is the key embedding dimension <code class="docutils literal notranslate"><span class="pre">kdim</span></code>.
See “Attention Is All You Need” for more details.</p>
</dd>
<dt>value: Value embeddings of shape <span class="math notranslate nohighlight">\((S, E_v)\)</span> for unbatched input, <span class="math notranslate nohighlight">\((S, N, E_v)\)</span> when</dt><dd><p><code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> or <span class="math notranslate nohighlight">\((N, S, E_v)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>, where <span class="math notranslate nohighlight">\(S\)</span> is the source
sequence length, <span class="math notranslate nohighlight">\(N\)</span> is the batch size, and <span class="math notranslate nohighlight">\(E_v\)</span> is the value embedding dimension <code class="docutils literal notranslate"><span class="pre">vdim</span></code>.
See “Attention Is All You Need” for more details.</p>
</dd>
<dt>key_padding_mask: If specified, a mask of shape <span class="math notranslate nohighlight">\((N, S)\)</span> indicating which elements within <code class="docutils literal notranslate"><span class="pre">key</span></code></dt><dd><p>to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched <cite>query</cite>, shape should be <span class="math notranslate nohighlight">\((S)\)</span>.
Binary and byte masks are supported.
For a binary mask, a <code class="docutils literal notranslate"><span class="pre">True</span></code> value indicates that the corresponding <code class="docutils literal notranslate"><span class="pre">key</span></code> value will be ignored for
the purpose of attention. For a byte mask, a non-zero value indicates that the corresponding <code class="docutils literal notranslate"><span class="pre">key</span></code>
value will be ignored.</p>
</dd>
<dt>need_weights: If specified, returns <code class="docutils literal notranslate"><span class="pre">attn_output_weights</span></code> in addition to <code class="docutils literal notranslate"><span class="pre">attn_outputs</span></code>.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt>attn_mask: If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape</dt><dd><p><span class="math notranslate nohighlight">\((L, S)\)</span> or <span class="math notranslate nohighlight">\((N\cdot\text{num\_heads}, L, S)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the batch size,
<span class="math notranslate nohighlight">\(L\)</span> is the target sequence length, and <span class="math notranslate nohighlight">\(S\)</span> is the source sequence length. A 2D mask will be
broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch.
Binary, byte, and float masks are supported. For a binary mask, a <code class="docutils literal notranslate"><span class="pre">True</span></code> value indicates that the
corresponding position is not allowed to attend. For a byte mask, a non-zero value indicates that the
corresponding position is not allowed to attend. For a float mask, the mask values will be added to
the attention weight.</p>
</dd>
<dt>average_attn_weights: If true, indicates that the returned <code class="docutils literal notranslate"><span class="pre">attn_weights</span></code> should be averaged across</dt><dd><p>heads. Otherwise, <code class="docutils literal notranslate"><span class="pre">attn_weights</span></code> are provided separately per head. Note that this flag only has an
effect when <code class="docutils literal notranslate"><span class="pre">need_weights=True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code> (i.e. average weights across heads)</p>
</dd>
</dl>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>attn_output</strong> - Attention outputs of shape <span class="math notranslate nohighlight">\((L, E)\)</span> when input is unbatched,
<span class="math notranslate nohighlight">\((L, N, E)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=False</span></code> or <span class="math notranslate nohighlight">\((N, L, E)\)</span> when <code class="docutils literal notranslate"><span class="pre">batch_first=True</span></code>,
where <span class="math notranslate nohighlight">\(L\)</span> is the target sequence length, <span class="math notranslate nohighlight">\(N\)</span> is the batch size, and <span class="math notranslate nohighlight">\(E\)</span> is the
embedding dimension <code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>.</p></li>
<li><p><strong>attn_output_weights</strong> - Only returned when <code class="docutils literal notranslate"><span class="pre">need_weights=True</span></code>. If <code class="docutils literal notranslate"><span class="pre">average_attn_weights=True</span></code>,
returns attention weights averaged across heads of shape <span class="math notranslate nohighlight">\((L, S)\)</span> when input is unbatched or
<span class="math notranslate nohighlight">\((N, L, S)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the batch size, <span class="math notranslate nohighlight">\(L\)</span> is the target sequence length, and
<span class="math notranslate nohighlight">\(S\)</span> is the source sequence length. If <code class="docutils literal notranslate"><span class="pre">average_weights=False</span></code>, returns attention weights per
head of shape <span class="math notranslate nohighlight">\((\text{num\_heads}, L, S)\)</span> when input is unbatched or <span class="math notranslate nohighlight">\((N, \text{num\_heads}, L, S)\)</span>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>batch_first</cite> argument is ignored for unbatched inputs.</p>
</div>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.model.transformer.encoder">
<span id="src-model-transformer-encoder-module"></span><h2>src.model.transformer.encoder module<a class="headerlink" href="#module-src.model.transformer.encoder" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.model.transformer.encoder.RelationalTransformerEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.model.transformer.encoder.</span></span><span class="sig-name descname"><span class="pre">RelationalTransformerEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_nested_tensor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_check</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.model.transformer.encoder.RelationalTransformerEncoder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.model.transformer.encoder.RelationalTransformerEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_key_padding_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#src.model.transformer.encoder.RelationalTransformerEncoder.forward" title="Permalink to this definition"></a></dt>
<dd><p>Pass the input through the encoder layers in turn.
Args:</p>
<blockquote>
<div><p>src: the sequence to the encoder (required).
mask: the mask for the src sequence (optional).
src_key_padding_mask: the mask for the src keys per batch (optional).</p>
</div></blockquote>
<dl class="simple">
<dt>Shape:</dt><dd><p>see the docs in Transformer class.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.model.transformer.encoder.RelationalTransformerEncoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.model.transformer.encoder.</span></span><span class="sig-name descname"><span class="pre">RelationalTransformerEncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">d_model:</span> <span class="pre">int,</span> <span class="pre">nhead:</span> <span class="pre">int,</span> <span class="pre">num_relations:</span> <span class="pre">int,</span> <span class="pre">dim_feedforward:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2048,</span> <span class="pre">dropout:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.1,</span> <span class="pre">activation:</span> <span class="pre">~typing.Union[str,</span> <span class="pre">~typing.Callable[[~torch.Tensor],</span> <span class="pre">~torch.Tensor]]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">relu&gt;,</span> <span class="pre">layer_norm_eps:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1e-05,</span> <span class="pre">batch_first:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">norm_first:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">device=None,</span> <span class="pre">dtype=None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#src.model.transformer.encoder.RelationalTransformerEncoderLayer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.model.transformer.encoder.RelationalTransformerEncoderLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_key_padding_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#src.model.transformer.encoder.RelationalTransformerEncoderLayer.forward" title="Permalink to this definition"></a></dt>
<dd><p>Pass the input through the encoder layer.
Args:</p>
<blockquote>
<div><p>src: the sequence to the encoder layer (required).
src_mask: the mask for the src sequence (optional).
src_key_padding_mask: the mask for the src keys per batch (optional).</p>
</div></blockquote>
<dl class="simple">
<dt>Shape:</dt><dd><p>see the docs in Transformer class.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.model.transformer.functional">
<span id="src-model-transformer-functional-module"></span><h2>src.model.transformer.functional module<a class="headerlink" href="#module-src.model.transformer.functional" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="src.model.transformer.functional.relational_multi_head_attention_forward">
<span class="sig-prename descclassname"><span class="pre">src.model.transformer.functional.</span></span><span class="sig-name descname"><span class="pre">relational_multi_head_attention_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relation_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim_to_check</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_proj_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_proj_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_zero_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_p</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_proj_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_proj_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_relation_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_relation_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_padding_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_separate_proj_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_proj_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_proj_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_proj_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">static_k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">static_v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_attn_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#src.model.transformer.functional.relational_multi_head_attention_forward" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>query, key, value: map a query and a set of key-value pairs to an output.</dt><dd><p>See “Attention Is All You Need” for more details.</p>
</dd>
</dl>
<p>embed_dim_to_check: total dimension of the model.
num_heads: parallel attention heads.
in_proj_weight, in_proj_bias: input projection weight and bias.
bias_k, bias_v: bias of the key and value sequences to be added at dim=0.
add_zero_attn: add a new batch of zeros to the key and</p>
<blockquote>
<div><p>value sequences at dim=1.</p>
</div></blockquote>
<p>dropout_p: probability of an element to be zeroed.
out_proj_weight, out_proj_bias: the output projection weight and bias.
training: apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
key_padding_mask: if provided, specified padding elements in the key will</p>
<blockquote>
<div><p>be ignored by the attention. This is an binary mask. When the value is True,
the corresponding value on the attention layer will be filled with -inf.</p>
</div></blockquote>
<p>need_weights: output attn_output_weights.
attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all</p>
<blockquote>
<div><p>the batches while a 3D mask allows to specify a different mask for the entries of each batch.</p>
</div></blockquote>
<dl class="simple">
<dt>use_separate_proj_weight: the function accept the proj. weights for query, key,</dt><dd><p>and value in different forms. If false, in_proj_weight will be used, which is
a combination of q_proj_weight, k_proj_weight, v_proj_weight.</p>
</dd>
</dl>
<p>q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.
static_k, static_v: static key and value used for attention operators.
average_attn_weights: If true, indicates that the returned <code class="docutils literal notranslate"><span class="pre">attn_weights</span></code> should be averaged across heads.</p>
<blockquote>
<div><p>Otherwise, <code class="docutils literal notranslate"><span class="pre">attn_weights</span></code> are provided separately per head. Note that this flag only has an effect
when <code class="docutils literal notranslate"><span class="pre">need_weights=True.</span></code>. Default: True</p>
</div></blockquote>
</dd>
<dt>Shape:</dt><dd><p>Inputs:
- query: <span class="math notranslate nohighlight">\((L, E)\)</span> or <span class="math notranslate nohighlight">\((L, N, E)\)</span> where L is the target sequence length, N is the batch size, E is</p>
<blockquote>
<div><p>the embedding dimension.</p>
</div></blockquote>
<ul class="simple">
<li><p>key: <span class="math notranslate nohighlight">\((S, E)\)</span> or <span class="math notranslate nohighlight">\((S, N, E)\)</span>, where S is the source sequence length, N is the batch size, E is
the embedding dimension.</p></li>
<li><p>value: <span class="math notranslate nohighlight">\((S, E)\)</span> or <span class="math notranslate nohighlight">\((S, N, E)\)</span> where S is the source sequence length, N is the batch size, E is
the embedding dimension.</p></li>
<li><p>key_padding_mask: <span class="math notranslate nohighlight">\((S)\)</span> or <span class="math notranslate nohighlight">\((N, S)\)</span> where N is the batch size, S is the source sequence length.
If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions
will be unchanged. If a BoolTensor is provided, the positions with the
value of <code class="docutils literal notranslate"><span class="pre">True</span></code> will be ignored while the position with the value of <code class="docutils literal notranslate"><span class="pre">False</span></code> will be unchanged.</p></li>
<li><p>attn_mask: 2D mask <span class="math notranslate nohighlight">\((L, S)\)</span> where L is the target sequence length, S is the source sequence length.
3D mask <span class="math notranslate nohighlight">\((N*num_heads, L, S)\)</span> where N is the batch size, L is the target sequence length,
S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked
positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code class="docutils literal notranslate"><span class="pre">True</span></code>
are not allowed to attend while <code class="docutils literal notranslate"><span class="pre">False</span></code> values will be unchanged. If a FloatTensor
is provided, it will be added to the attention weight.</p></li>
<li><p>static_k: <span class="math notranslate nohighlight">\((N*num_heads, S, E/num_heads)\)</span>, where S is the source sequence length,
N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.</p></li>
<li><p>static_v: <span class="math notranslate nohighlight">\((N*num_heads, S, E/num_heads)\)</span>, where S is the source sequence length,
N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.</p></li>
</ul>
<p>Outputs:
- attn_output: <span class="math notranslate nohighlight">\((L, E)\)</span> or <span class="math notranslate nohighlight">\((L, N, E)\)</span> where L is the target sequence length, N is the batch size,</p>
<blockquote>
<div><p>E is the embedding dimension.</p>
</div></blockquote>
<ul class="simple">
<li><p>attn_output_weights: Only returned when <code class="docutils literal notranslate"><span class="pre">need_weights=True</span></code>. If <code class="docutils literal notranslate"><span class="pre">average_attn_weights=True</span></code>, returns
attention weights averaged across heads of shape <span class="math notranslate nohighlight">\((L, S)\)</span> when input is unbatched or
<span class="math notranslate nohighlight">\((N, L, S)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the batch size, <span class="math notranslate nohighlight">\(L\)</span> is the target sequence length, and
<span class="math notranslate nohighlight">\(S\)</span> is the source sequence length. If <code class="docutils literal notranslate"><span class="pre">average_attn_weights=False</span></code>, returns attention weights per
head of shape <span class="math notranslate nohighlight">\((num_heads, L, S)\)</span> when input is unbatched or <span class="math notranslate nohighlight">\((N, num_heads, L, S)\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-src.model.transformer">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-src.model.transformer" title="Permalink to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, hyukkyu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>